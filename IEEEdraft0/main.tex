\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[hyphens]{url}
\usepackage{tikz}
\usepackage[inline]{enumitem}
\usepackage{pifont} 
\usepackage{listings}
\graphicspath{ {./images/} }
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
        \node[shape=circle,draw,inner sep=.6pt] (char) {#1};}}
\newcommand{\FedCampus}{\textsc{FedCampus}}
\newcommand{\FedKit}{\textsc{FedKit}}
\newcommand{\challa}{\circled{$\boldsymbol\alpha$}}
\newcommand{\challb}{\circled{$\boldsymbol\beta$}}
\newcommand{\challc}{\circled{$\boldsymbol\gamma$}}

\title
{\FedKit{}: Enabling Cross-Platform Federated Learning for Android and iOS}

\author{\IEEEauthorblockN{Sichang He\IEEEauthorrefmark{1}, Beilong Tang\IEEEauthorrefmark{1}, Boyan Zhang\IEEEauthorrefmark{1}, Jiaoqi Shao\IEEEauthorrefmark{1}\IEEEauthorrefmark{2}, Xiaomin Ouyang\IEEEauthorrefmark{3}, Daniel\,Nata Nugraha\IEEEauthorrefmark{4}, Bing Luo\IEEEauthorrefmark{1}}
    \IEEEauthorblockA{\IEEEauthorrefmark{1}% Data Science Research Center,
        Duke Kunshan University (DKU), Jiangsu, China,
    % }
    % \IEEEauthorblockA{
        \IEEEauthorrefmark{2}% Department of Electronic and Computer Engineering,
        The Hong Kong University of Science and Technology, China,
    }
    \IEEEauthorblockA{
        \IEEEauthorrefmark{3}% Department of Electrical and Computer Engineering,
        University of California, Los Angeles, USA,
    % }
    % \IEEEauthorblockA{
        \IEEEauthorrefmark{4}Flower Labs GmbH, Winterhuder Weg 29, 22085 Hamburg, Germany
    }% <-this % stops an unwanted space
}

\maketitle

\begin{abstract}
    We present \FedKit{}, a federated learning (FL) system tailored for
    cross-platform FL research on \textit{Android and iOS} devices.
    \FedKit{} pipelines FL development by
    enabling machine learning (ML) model conversion,
    hardware-accelerated training,
    and cross-platform model aggregation.
    Our FL workflow supports flexible ML operations (MLOps) in production,
    facilitating continuous model delivery and training.
    We have deployed \FedKit{} in a real-world use case for
    exercise data analysis on DKU campus,
    demonstrating its effectiveness.
    \FedKit{} is open-source at \url{https://github.com/FedCampus/FedKit}.
\end{abstract}

% \begin{IEEEkeywords}
% \end{IEEEkeywords}

\section{Motivation and Analysis}

FL is a promising technique to collaboratively train shared ML models on
end devices while retaining private local data~\cite{wang2023federated}.
However, most mobile FL systems cater to simulations on desktop computers.
To guide FL algorithm design with real-world statistics,
we proposed the \FedCampus{} project to build a mobile FL platform for
harnessing real users' health data on university campuses.

Existing accessible FL systems fall short of \FedCampus{}' requirements,
as we outline in Table~\ref{tbl:fn-systems}.
Specifically, we identify three key challenges:
\challa~to collaboratively train the same models across
our users' diverse smartphones,
we require cross-platform on-device training and
cross-platform model aggregation;
\challb~to facilitate model development,
we require a friendly development environment for our data scientists;
\challc~to deploy new models on our mobile apps in production,
we require a customizable and streamlined deployment procedure.

\section{Proposed Solution}

\FedKit{} addresses challenges \challa{} and \challb{} by
a cross-platform (Android-iOS) FL pipeline centered around ML models in
Section~\ref{sec:pipeline},
and addresses challenge \challc{} by flexible MLOps in Section~\ref{sec:mlops}.
As an overview,
\FedKit{} facilitates FL among Android and iOS client apps,
coordinated by a single Backend server.
Each client trains a local model with private data,
and the Backend performs cross-platform aggregation of these local models to update the global model.

\subsection{Cross-Platform FL Model Pipeline}
\label{sec:pipeline}

To enable cross-platform FL,
especially \textit{cross-platform aggregation},
we propose a pipeline comprising
\textit{model conversion} and
\textit{unified training APIs},
as shown in Fig.~\ref{cross_fl}.

\begin{table}
    \centering
    \small
    % \setlength{\tabcolsep}{2pt}
    \begin{tabular}{lccccc}
        Functionality         & \cite{he2020fedml}
                                          & \cite{madrigal2023project,lai2022fedscale}
                                                      & \cite{beutel2020flower,mathur2021ondevice}
                                                                  & \FedKit{} \\
        \hline
        Android-Only          & \ding{51} & \ding{51} & \ding{51} & \ding{51}       \\
        iOS-Only              & \ding{55} & \ding{55} & \ding{51} & \ding{51}       \\
        Cross-Platform Aggregation
                              & \ding{55} & \ding{55} & \ding{55} & \ding{51}       \\
        \hline
        Training Acceleration & \ding{51} & \ding{51} & \ding{51} & \ding{51}       \\
        MLOps                 & \ding{51} & \ding{51} & \ding{55} & \ding{51}       \\
        Open-Source Backend   & \ding{55} & \ding{55} & \ding{51} & \ding{51}       \\
    \end{tabular}
    \caption{Functionality Comparison among On-Smartphone FL Systems.
    }
    \label{tbl:fn-systems}
\end{table}

\begin{figure}
    \centering
    \includegraphics*[width=\linewidth]{model_pipeline.pdf}
    \caption{\FedKit{} Model Pipeline for Cross-Platform FL.}
    \label{cross_fl}
\end{figure}

\subsubsection{Model Conversion}
We begin in Python by converting models into formats compatible with
Android (TensorFlow Lite or TFLite) and iOS (Core ML).
Core ML defines a fixed model structure and provides
the official converter CoreMLTools.
For TFLite, we \textit{standardized} a model format, and then
developed a compliant TensorFlow converter.
This standardized format includes
four essential FL methods
(\lstinline{train}, \lstinline{infer}, \lstinline{parameters},
and \lstinline{restore}).


\subsubsection{Unified Training APIs}
\FedKit{} provides \textit{TFLite Trainer} and \textit{Core ML Trainer} to
train the converted models on Android and iOS devices,
utilizing GPU and NPU acceleration.
Moreover, both trainers expose unified APIs for
\textit{retrieving and setting model parameters,
    model fitting, and evaluation}.
On Android, these APIs invoke the TFLite interpreter to call
our \textit{standardized methods} defined in \textit{Model Conversion}.
However, on iOS, our experimentation revealed that
Core ML \textit{forbids} directly setting parameters, which could impede FL.
To overcome this constraint,
we modify the underlying ProtoBuf representations of
Core ML models.
Specifically,
we employ Swift code generated from the relevant ProtoBuf definition files,
and navigate nested model definitions to access parameters on iPhones.
Consequently, our unified training APIs exhibit comparable functionality on
both iOS and Android platforms.


\subsubsection{Cross-Platform Aggregation}
Aggregation necessitates
\textit{uniform parameter representations},
which poses primary challenges in
retrieving and setting parameters for TFLite and Core ML.
1)
The TFLite interpreter only accepts inputs/outputs as maps from \textit{names} to
\textit{tensors}.
Therefore, during \textit{Model Conversion},
we assign index-based names to each parameter layer and
dynamically generate the concrete methods that accept these arguments.
During training, we call the methods with these index-based names to
access parameters.
2)
Core ML permits \textit{only} specific layers to be \textit{updatable} and
only provides their parameters post-training.
Thus, to obtain \textit{other layers'} parameters,
we implemented a solution using ProtoBuf manipulation.
This approach involves recording layer information
during \textit{Model Conversion} and
utilizing it during training.
Finally, these unified parameters enable seamless cross-platform aggregation.

\subsection{Flexible MLOps in Production}
\label{sec:mlops}
\newcommand{\model}{$M$}
\newcommand{\fs}{$S_\mathrm F$}
\FedKit{} empowers researchers to deploy models and algorithms continuously (MLOps)
in production.
Leveraging our complete control of the self-hosted Backend,
\FedKit{}'s three-step FL workflow
facilitates continuous delivery and training,
as illustrated in Fig.~\ref{fig:fl-workflow}.
\subsubsection{Continuous Cross-Platform Model Delivery}
Traditionally, models for on-device training are \textit{embedded} into client apps.
However, this approach couples model delivery with app updates,
resulting in complexities when submitting apps to app stores and garnering user adoption.

Circumventing this complexity,
\FedKit{} enables \textit{continuous model delivery without app updates} by
decoupling models from clients through \textit{Model Request},
which allows new model deployment by uploading to the Backend.
Specifically, clients request the Backend for a model (TFLite/Core ML)
aligned with
their platform (Android/iOS) and training data type.
The Backend selects the appropriate model \model{} from the database and
responds with detailed model information.
Consequently, \textit{Model Request} delivers the TFLite or Core ML model
to clients for FL training.

\subsubsection{Customizable Continuous FL Training}
\FedKit{} manages continuous FL training by allowing \textit{multiple parallel FL training sessions}
through \textit{FL Server Setup}.
When clients request an FL Server to
train their chosen model \model{},
the Backend either reuses a suitable FL Server \fs{} if it exists,
or spawns a new one.
Each FL Server
operates as an independent Python subprocess of the Django Backend,
occupying its own port that
clients connect to for FL Training.
This dynamic approach ensures that
newly delivered models can be immediately trained with new FL Servers
without affecting ongoing ones.
Furthermore,
these FL Servers employ the Flower FL Framework for
scheduling training and evaluation.
This decision empowers our FL Servers to leverage Flower's flexibility and
allow for FL algorithm customizations in Python.

\begin{figure}
    \centering
    \includegraphics*[width=0.9\linewidth]{fl_workflow.pdf}
    \caption{\FedKit{} FL Workflow.}
    \label{fig:fl-workflow}
\end{figure}

\section{Use Case: FedCampus}

\begin{figure}
    \centering
    \includegraphics*[width=0.9\linewidth]{fedcampus.pdf}
    \caption{FedCampus Experiment Setup.}
    \label{fig:fedcampus}
\end{figure}

We applied \FedKit{} to a practical use case, FedCampus,
utilizing a mobile app to harness exercise data on a university campus.
This context demanded Android, iOS, and cross-platform aggregation support.
Additionally,
FL on mobile devices necessitates training acceleration for
a smooth user experience
and MLOps to facilitate iterative model design.

In the FedCampus experiment setup (Fig.~\ref{fig:fedcampus}),
we self-hosted the Backend
and displayed the real-time logs and losses on a connected laptop.
Our app, powered by \FedKit{},
performed FL across Android and iOS devices,
training a sleep-efficiency prediction model
akin to \cite{khoa2022fedmcrnn}
using data from the smartwatches.
The results showcased a significant reduction in the model's training loss,
demonstrating \FedKit{}'s effectiveness in real-world scenarios.

% \section*{Acknowledgments}
% We would like to thank Flower Labs for the discussion on on-device training.

\bibliographystyle{IEEEtran}
\bibliography{main}
\vspace{12pt}
\color{red}

\end{document}
